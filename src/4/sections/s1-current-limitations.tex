\subsection{Limitations}
\label{sec:current-limitations}
% Short intro
The bottom-up characterization method helps create higher-order models of circuit functions.
However, preliminary tests regarding this method are inconclusive.
Multiple sources of errors in the modeling method can be found to explain the differences observed.

\subsubsection{Impact of characterization output load and output modeling method}

% First source of error - load impedance
Previously, it was indicated that all characterization curves were extracted with a fixed output load.
In \ref{sec:application-test-vehicle}, the output load Zout was 1M\textOmega.
It is believed that this is the first major source of error.

% Compare 1Mohm with real case where blocks are connected together
%TODO: Clarify
In reality, each block (pre-regulator, bandgap, regulator) sees a load impedance on its output much different than 1M\textOmega.
For instance, the output of the pre-regulator is a supply.
As such, it can deliver about 20mA of current while maintaining 8V.
Thus, the minimum output load this block can sustain is 400\textOmega.
The bandgap, on the other hand, provides a reference voltage at 1V but does not deliver a lot of DC current.
More than 1uA is enough to make the output fall of a hundred millivolts.
In this case, the bandgap must see an output impedance of at least 1M\textOmega.

% What is done next
To evaluate the impact the pre-regulator is characterized again, this time with lower load values.
Variations on the input stresses and results are summarized in table \ref{tab:impact-load-on-cz}.

% Analyse the table - worst case
For the smallest 10V stress amplitude, the failure time is largely impacted by the output load.
This is especially true for long pulses.
The worst case is for the -10V 1\textmugreek{}s pulse.
The output goes below 0V for 1330 ns with 500\textOmega\ on the output, but with 1M\textOmega\, no failure is observed.

% Analyse the table - other cases
However, it is interesting to notice that for larger pulse amplitudes, the output load has a limited impact on the failure duration.
It seems that once the output is at fault, having 500\textOmega\ or 1M\textOmega\ connected to it doesn't change how long it remains at fault.
Fig. \ref{fig:impact-time-domain-load} provides a visual representation of the phenomenon, to try to explain why this result is obtained.
When the output is disturbed and its amplitude is near the failure criteria, the load value has a strong impact on the width of the failure.
Indeed, a large load value (1 M\textOmega) causes the output to be slightly above the failure criteria, thus no failure is recorded.
On the contrary, with a small load (500 \textOmega\) the output amplitude has changed a little bit and is now below the failure criteria.
It is interesting to notice that in the reference simulations, the width of the disturbance doesn't change much with the load.

%TODO: To do with more blocks

\begin{table}[!p]
\centering
\begin{tabular}{llll}
\toprule
load (\textOmega) & amplitude (V) & length (ns) & output disturbed (ns)   \\ \midrule
500               & -10        & 10         & 10n    \\
5k                &            &            & 1n    \\
50k               &            &            & None    \\
1M                &            &            & None    \\
\rowcolor[gray]{.95}
500               &            & 100        & 100n    \\ \rowcolor[gray]{.95}
5k                &            &            & 1n    \\ \rowcolor[gray]{.95}
50k               &            &            & None    \\ \rowcolor[gray]{.95}
1M                &            &            & None    \\

500               &            & 1000       & 1330n    \\
5k                &            &            & 1289n    \\
50k               &            &            & None    \\
1M                &            &            & None     \\
\rowcolor[gray]{.95}
500               & -30        & 10         & 20n    \\ \rowcolor[gray]{.95}
5k                &            &            & 10n    \\ \rowcolor[gray]{.95}
50k               &            &            & 10n    \\ \rowcolor[gray]{.95}
1M                &            &            & 10n    \\

500               &            & 100        &  506n   \\
5k                &            &            &  580n   \\
50k               &            &            &  594n   \\
1M                &            &            &  594n   \\
\rowcolor[gray]{.95}
500               &            & 1000       & 2087    \\ \rowcolor[gray]{.95}
5k                &            &            & 2194    \\ \rowcolor[gray]{.95}
50k               &            &            & 2206    \\ \rowcolor[gray]{.95}
1M                &            &            & 2206    \\

500               & -45        & 10         & 46n    \\
5k                &            &            & 10n    \\
50k               &            &            & 10n    \\
1M                &            &            & 10n    \\
\rowcolor[gray]{.95}
500               &            & 100        & 657n    \\ \rowcolor[gray]{.95}
5k                &            &            & 715n    \\ \rowcolor[gray]{.95}
50k               &            &            & 717n    \\ \rowcolor[gray]{.95}
1M                &            &            & 727n    \\

500               &            & 1000       & 2668n    \\
5k                &            &            & 2764n   \\
50k               &            &            & 2800n    \\
1M                &            &            & 2800n    \\

\bottomrule
\end{tabular}
\caption{Impact of the output load on characterization results}
\label{tab:impact-load-on-cz}
\end{table}

\begin{figure}
  \centering
  \includegraphics{src/4/figures/zout_impact_time_domain.pdf}
  \caption{Represented impact of output load impedance on the output waveform during a disturbance with a small amplitude (green) and a large amplitude (red)}
  \label{fig:impact-time-domain-load}
\end{figure}

% Conclusion, the main impact is not the characterization load, but the single failure criteria
In conclusion, the load value used during characterization does not seem to be the main source of error.
Rather, it is the consequence of using a single level as failure criteria, which eliminates a lot of information.
Basically, it oversimplifies the waveform of the output.
This is illustrated by figure. \ref{fig:impact-single-failure-criteria}.
Any disturbance under the failure criteria is modeled as no disturbance.
This is the case even if the disturbance is just below the failure criteria, without crossing it.
On the contrary, any disturbance beyond the failure criteria uses the failure level for modeling.
The max value of the output may be much larger than the failure criteria, however, it will still be modeled as a square pulse of amplitude Vfail.

\begin{figure}[!h]
  \centering
  \includegraphics{src/4/figures/bad_output_modelling.pdf}
  \caption{Lack of accuracy caused by the use of a single failure criteria to model the output}
  \label{fig:impact-single-failure-criteria}
\end{figure}

% Talk about static impedances vs dynamic
%TODO: Not studied ?
% Talk about real vs imaginary impedances
%TODO: Not studied

\subsubsection{Other sources}

%TODO: Make section ? Speak about limitations with multiples nets that are responsible for error propagation
%TODO: Limitation with multiple nets. Interactions between inputs. Diamond-like connections

% What was the promise of this model
This method looked rather promising in terms of applicability.
In theory, a block could be characterized once, and its model reused in different places.
The robustness of a full system could be quickly and easily deduced from the models of its parts.

% Real outcome is that it's not working well
However, with the study case exposed earlier, several issues arose that clearly limit the ability of the model to perform as expected.

% Main issue
So far the main issue of this method is to be limited to a binary fail/no fail criteria.
In some cases, the specification could be used to set this criteria, but mostly it was an arbitrary level.
For digital cells, a binary criteria is correct because above a certain input level disturbance an output can be switched, and the failure is then clear.
However, for most analog functions, there is no clear failure.
Most nets will have degraded values until extreme levels where biasing might completely fail.
Sometimes, the product is destroyed before reaching those extreme levels.
In any case, the binary criteria hides a lot of information about the degradation.

% Secondary issue
It was also suspected that the output load used during characterization might impact the results too much.
In practice, it was proven in the case of the regulator that this is not exactly true.
Changing the load can induce a varation, but seemingly more limited than the impact of the binary failure criteria.

% Directivity ?
% Basically : WE ASSUME STRESS AND FAILURES PROPAGATE FROM INPUT TO OUTPUT. MAY NOT BE THE CASE. ALSO, MAY NOT WORK IN REVERSE WITH SINGLE2MANY BLOCK CONNECTIONS

% Next
The next section will explore a modification of the characterization method to fix the binary failure criteria issue.
